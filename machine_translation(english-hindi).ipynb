{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6SlRAZyv9zDe5DT+a9MF3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, dot, Activation, concatenate, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "wAhKU91cw4Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset.csv')"
      ],
      "metadata": {
        "id": "Gw7eeqazLZtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "XGzJcouhLgsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "htX0BAbpLkBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxTXUzSdLl-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['english_sentence'] = df['english_sentence'].astype(str).fillna('')\n",
        "df['hindi_sentence'] = df['hindi_sentence'].astype(str).fillna('')"
      ],
      "metadata": {
        "id": "li_qGyUOrch8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences = df['english_sentence'].tolist()\n",
        "hindi_sentences = df['hindi_sentence'].tolist()"
      ],
      "metadata": {
        "id": "N8vMa-1Dr78R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(english_sentences)\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
        "max_eng_len = max(len(seq) for seq in eng_sequences)\n",
        "eng_padded = pad_sequences(eng_sequences, padding='post', maxlen=max_eng_len)\n",
        "\n"
      ],
      "metadata": {
        "id": "0UVRbRlZrGgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded.shape"
      ],
      "metadata": {
        "id": "yraeu4MBAj35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hin_tokenizer = Tokenizer()\n",
        "hin_tokenizer.fit_on_texts(hindi_sentences)\n",
        "hin_sequences = hin_tokenizer.texts_to_sequences(hindi_sentences)\n",
        "max_hin_len = max(len(seq) for seq in hin_sequences)\n",
        "hin_padded = pad_sequences(hin_sequences, padding='post', maxlen=max_hin_len)"
      ],
      "metadata": {
        "id": "U30e9EKyrCtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len"
      ],
      "metadata": {
        "id": "2TxZymRr-4bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_hin_len"
      ],
      "metadata": {
        "id": "7j4hUQv6_lsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "ck-dYv8-znHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size"
      ],
      "metadata": {
        "id": "irzD4UTV-f3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "encoder_inputs = Input(shape=(max_eng_len,))\n",
        "encoder_embedding = Embedding(input_dim=eng_vocab_size, output_dim=100, input_length=max_eng_len, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm, state_h, state_c = LSTM(256, return_state=True, return_sequences=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n"
      ],
      "metadata": {
        "id": "6pSRTZ7CzY0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embedding.shape"
      ],
      "metadata": {
        "id": "xHBg2-bL9J4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_layer(encoder_outputs, decoder_outputs):\n",
        "    # Dot product between decoder outputs and encoder outputs\n",
        "    attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])\n",
        "    attention = Activation('softmax')(attention)\n",
        "    context = dot([attention, encoder_outputs], axes=[2, 1])\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "TJDlz1ZT-XNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(input_dim=hin_vocab_size, output_dim=100, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm, _, _ = LSTM(256, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)"
      ],
      "metadata": {
        "id": "qIn3_B9sDVfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = attention_layer(encoder_lstm, decoder_lstm)"
      ],
      "metadata": {
        "id": "LjXdTGC3EBmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_combined_context = concatenate([context, decoder_lstm])"
      ],
      "metadata": {
        "id": "jjJPY35wEKRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_dense = TimeDistributed(Dense(hin_vocab_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_combined_context)"
      ],
      "metadata": {
        "id": "ya3Ybc-2EQ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Uf2QjjZBEegx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift target sequences for teacher forcing\n",
        "hin_sequences_input =hin_padded[:, :-1]\n",
        "hin_sequences_output = hin_padded[:, 1:]\n",
        "\n",
        "# Expand dimensions to fit sparse categorical cross-entropy loss\n",
        "hin_sequences_output = np.expand_dims(hin_sequences_output, -1)\n"
      ],
      "metadata": {
        "id": "IReOw_DeErQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded = np.array(eng_padded)\n",
        "hin_sequences_input = np.array(hin_sequences_input)\n",
        "hin_sequences_output = np.array(hin_sequences_output)"
      ],
      "metadata": {
        "id": "nN9sFeRc4E5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0PyrjtaJ3gVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train, eng_val, hin_input_train, hin_input_val, hin_output_train, hin_output_val = train_test_split(\n",
        "    eng_padded, hin_sequences_input, hin_sequences_output, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "YKDXb9h83Zf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    [eng_train, hin_input_train],\n",
        "    hin_output_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_data=([eng_val, hin_input_val], hin_output_val)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "oAaFOU00FLfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OeVHypsDLScY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}